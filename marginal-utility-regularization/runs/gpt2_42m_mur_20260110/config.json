{
  "dataset_name": "HuggingFaceFW/fineweb-edu",
  "dataset_config": null,
  "text_column": "text",
  "train_split": "train",
  "eval_split": "train",
  "batch_size": 32,
  "eval_batch_size": 2,
  "learning_rate": 0.001,
  "weight_decay": 0.1,
  "num_train_steps": 10000,
  "eval_steps": 1000,
  "save_steps": 10000,
  "log_steps": 50,
  "gradient_accumulation_steps": 1,
  "max_grad_norm": 1.0,
  "seed": 42,
  "tokenizer_name": "meta-llama/Llama-2-7b-hf",
  "run_name": "gpt2_42m_mur_20260110",
  "output_dir": "runs/gpt2_42m_mur_20260110",
  "model": {
    "arch": "gpt2",
    "vocab_size": 32000,
    "hidden_size": 512,
    "num_layers": 8,
    "num_heads": 8,
    "max_position_embeddings": 1024,
    "attn_dropout": 0.1,
    "resid_dropout": 0.1,
    "embd_dropout": 0.1,
    "tie_word_embeddings": true,
    "intermediate_size": 2048,
    "bos_token_id": 1,
    "eos_token_id": 2
  },
  "mur": {
    "enabled": true,
    "mode": "update",
    "metric": "cos",
    "mid_start": 0.33,
    "mid_end": 0.67,
    "tau": 0.1,
    "lambda_max": 0.0,
    "alpha": 0.2,
    "warmup_steps": 1000,
    "ramp_steps": 2000,
    "token_reduce": "sample_k",
    "sample_k": 32,
    "eps": 1e-06,
    "fp32_dot": true,
    "log_layer_stats": false
  },
  "max_train_tokens": 1000000000,
  "streaming_shuffle_buffer": 0,
  "seq_len": 512,
  "bf16": true,
  "warmup_ratio": 0.1,
  "wandb_enabled": true,
  "project_name": "marginal-utility-regularization"
}