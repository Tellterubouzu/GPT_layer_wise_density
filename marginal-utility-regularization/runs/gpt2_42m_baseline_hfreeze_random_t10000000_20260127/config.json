{
  "dataset_name": "HuggingFaceFW/fineweb-edu",
  "dataset_config": null,
  "text_column": "text",
  "train_split": "train",
  "eval_split": "train",
  "batch_size": 32,
  "eval_batch_size": 2,
  "learning_rate": 0.001,
  "weight_decay": 0.1,
  "num_train_steps": 10000,
  "eval_steps": 1000,
  "save_steps": 10000,
  "log_steps": 50,
  "gradient_accumulation_steps": 1,
  "max_grad_norm": 1.0,
  "seed": 42,
  "tokenizer_name": "meta-llama/Llama-2-7b-hf",
  "run_name": "gpt2_42m_baseline_hfreeze_random_t10000000_20260127",
  "output_dir": "runs/gpt2_42m_baseline_hfreeze_random_t10000000_20260127",
  "model": {
    "arch": "gpt2",
    "vocab_size": 32000,
    "hidden_size": 512,
    "num_layers": 8,
    "num_heads": 8,
    "max_position_embeddings": 1024,
    "attn_dropout": 0.1,
    "resid_dropout": 0.1,
    "embd_dropout": 0.1,
    "tie_word_embeddings": true,
    "intermediate_size": 2048,
    "bos_token_id": 1,
    "eos_token_id": 2
  },
  "mur": {
    "enabled": false
  },
  "max_train_tokens": 1000000000,
  "streaming_shuffle_buffer": 0,
  "seq_len": 256,
  "bf16": true,
  "warmup_ratio": 0.1,
  "wandb_enabled": true,
  "hierarchical_freeze": {
    "mode": "random",
    "unfreeze_tokens": 10000000,
    "enabled": true,
    "seed": 42,
    "order": [
      3,
      4,
      6,
      7,
      2,
      5,
      0,
      1
    ]
  },
  "freeze": {
    "enabled": false
  },
  "project_name": "marginal-utility-regularization"
}